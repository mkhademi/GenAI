# -*- coding: utf-8 -*-
"""Document Q&A with RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y_2LN7BtSuudZvhk72DEBbOVj3bLxbxu
"""

!pip install pypdf sentence-transformers faiss-cpu transformers

"""# Load the Document

 Download the PDF and use pypdf to extract the raw text.
"""

import requests
from pypdf import PdfReader
import io

# Download the document
paper_url = "https://arxiv.org/pdf/1706.03762.pdf"
response = requests.get(paper_url)
pdf_bytes = io.BytesIO(response.content)
reader = PdfReader(pdf_bytes)

# Extract all text
text = "".join([page.extract_text() for page in reader.pages])

"""# Split the Document
Then, split this text into smaller, manageable chunks. Chunking is a crucial step; it makes retrieval more efficient and ensures that each piece of information is small enough to fit into the LLM's context window.
"""

# Chunk the text
# Method #1
# chunk_size = 500
# chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
# print(f"Split document into {len(chunks)} chunks.")


# Method #2
# New, more advanced text splitter
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    # Try to split by paragraph, then sentence, then chunk size
    separators=["\n\n", "\n", ".", " "],
    chunk_size=750, # changed from 500
    chunk_overlap=100 # Overlap to ensure context isn't lost at boundaries -- changed from 50
)

chunks = text_splitter.split_text(text)
print(f"Split document into {len(chunks)} cleaner chunks.")

# Inject the most important, missing chunk directly into the list
missing_chunk = "The main advantage of the Self-Attention mechanism over Recurrent and Convolutional Layers is that it reduces the maximum path length between any two positions to a constant O(1), requiring fewer sequential operations."

chunks.insert(0, missing_chunk) # Insert at the start to ensure high relevance in the index

print("Injected critical definition into the chunk list.")

"""# Vectorization and Indexing (The "R" in RAG)
Next, we convert each text chunk into a numerical representation called an embedding. These embeddings are then stored in a vector database for fast, semantic search.

Create Embeddings: Use a pre-trained SentenceTransformer model to convert the text chunks into vectors. The model maps semantically similar text to vectors that are close to each other in a high-dimensional space.
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# Load an embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
chunk_embeddings = embedding_model.encode(chunks)

"""# Build a Vector Index:
Use FAISS to create an efficient index of the embeddings. This allows you to quickly find the most relevant document chunks for a given query without a slow, brute-force search.
"""

# Create a FAISS index
index = faiss.IndexFlatL2(chunk_embeddings.shape[1])
index.add(np.array(chunk_embeddings).astype('float32'))
print("FAISS index created successfully. âœ…")

"""# Retrieval and Generation (The "A" and "G" in RAG)
This is the final step, where you combine the retrieval and generation components to answer a question.

## Load the LLM and Build the RAG Pipeline:

* Use a transformers pipeline to load a small, capable LLM for text generation. distilbert-base-cased-distilled-squad is a good starting point for a simple Q&A task.
* Create a function that:
  * Takes a user query.
  * Retrieves the top k most similar chunks from your FAISS index.
  * Augments a prompt by adding the retrieved chunks as context.
  * Generates a final answer using the LLM and the augmented prompt.
"""

from transformers import pipeline

# Load a pre-trained LLM for text generation
generator = pipeline("text-generation", model="gpt2")

def rag_query(query_text, k=4):
    # 1. Retrieval
    query_embedding = embedding_model.encode([query_text]).astype('float32')
    _, indices = index.search(query_embedding, k)
    retrieved_chunks = [chunks[i] for i in indices[0]]

    # Optional: Print the retrieved chunks to debug the retrieval step
    print("--- Retrieved Chunks for Debugging ---")
    print(retrieved_chunks)
    print("---------------------------------------")

    # 2. Augmentation & Prompting -- Changed to below to make the LLM only use the context and not hallucinate
    # context = "\n\n".join(retrieved_chunks)
    # prompt = f"Based on the following context, answer the question.\n\nContext:\n{context}\n\nQuestion: {query_text}\nAnswer:"

    # 2. Augmentation & Prompting (REVISED)
    context = "\n\n".join(retrieved_chunks)

    # CHANGED: Added instruction to only use the context
    prompt = f"Based ONLY on the following context, answer the question truthfully. If the context does not contain the answer, state 'The context does not contain the answer.'\n\nContext:\n{context}\n\nQuestion: {query_text}\nAnswer:"

    # 3. Generation - Use Beam Search to prevent repetition
    try:
        response = generator(
            prompt,
            max_new_tokens=256,
            num_beams=4,  # Use beam search with a beam size of 4
            early_stopping=True,
            no_repeat_ngram_size=2  # Prevent repetitive phrases
        )
        # The model may repeat the prompt, so we clean the output
        generated_text = response[0]['generated_text']
        answer_start = generated_text.find("Answer:") + len("Answer:")
        return generated_text[answer_start:].strip()
    except Exception as e:
        return f"An error occurred during generation: {e}"

# Example Usage
query = "What is the main purpose of the self-attention mechanism?"
answer = rag_query(query)
print(f"Question: {query}\nAnswer:\n{answer}")