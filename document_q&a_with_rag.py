# -*- coding: utf-8 -*-
"""Document Q&A with RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hfDRMWDxMC6-JraAB3CGONjlp9t76z7P
"""

!pip install pypdf sentence-transformers faiss-cpu transformers

"""# Load and Split the Document

 Download the PDF and use pypdf to extract the raw text. Then, split this text into smaller, manageable chunks. Chunking is a crucial step; it makes retrieval more efficient and ensures that each piece of information is small enough to fit into the LLM's context window.
"""

import requests
from pypdf import PdfReader
import io

# Download the document
paper_url = "https://arxiv.org/pdf/1706.03762.pdf"
response = requests.get(paper_url)
pdf_bytes = io.BytesIO(response.content)
reader = PdfReader(pdf_bytes)

# Extract all text
text = "".join([page.extract_text() for page in reader.pages])

# Chunk the text
chunk_size = 500
chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
print(f"Split document into {len(chunks)} chunks.")

"""# Step 2: Vectorization and Indexing (The "R" in RAG)
Next, we convert each text chunk into a numerical representation called an embedding. These embeddings are then stored in a vector database for fast, semantic search.

Create Embeddings: Use a pre-trained SentenceTransformer model to convert the text chunks into vectors. The model maps semantically similar text to vectors that are close to each other in a high-dimensional space.


"""

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# Load an embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
chunk_embeddings = embedding_model.encode(chunks)

"""# Build a Vector Index:
Use FAISS to create an efficient index of the embeddings. This allows you to quickly find the most relevant document chunks for a given query without a slow, brute-force search.
"""

# Create a FAISS index
index = faiss.IndexFlatL2(chunk_embeddings.shape[1])
index.add(np.array(chunk_embeddings).astype('float32'))
print("FAISS index created successfully. âœ…")

"""# Step 3: Retrieval and Generation (The "A" and "G" in RAG)
This is the final step, where you combine the retrieval and generation components to answer a question.

## Load the LLM:
Use a transformers pipeline to load a small, capable LLM for text generation. distilbert-base-cased-distilled-squad is a good starting point for a simple Q&A task.
"""

from transformers import pipeline

# Load a pre-trained LLM for text generation
# We're switching to distilgpt2, which is designed for text generation
generator = pipeline("text-generation", model="distilgpt2")

"""## Build the RAG Pipeline: Create a function that:

Takes a user query.

Retrieves the top k most similar chunks from your FAISS index.

Augments a prompt by adding the retrieved chunks as context.

Generates a final answer using the LLM and the augmented prompt.
"""

def rag_query(query_text, k=2):
    # 1. Retrieval
    query_embedding = embedding_model.encode([query_text]).astype('float32')
    _, indices = index.search(query_embedding, k)
    retrieved_chunks = [chunks[i] for i in indices[0]]

    # 2. Augmentation & Prompting
    context = "\n\n".join(retrieved_chunks)
    prompt = f"Based on the following context, answer the question.\n\nContext:\n{context}\n\nQuestion: {query_text}\nAnswer:"

    # 3. Generation
    response = generator(prompt, max_length=512, truncation=True)
    return response[0]['generated_text']

# Example Usage
query = "What is the main purpose of the self-attention mechanism?"
answer = rag_query(query)
print(f"Question: {query}\nAnswer:\n{answer}")