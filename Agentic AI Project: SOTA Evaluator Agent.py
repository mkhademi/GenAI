# -*- coding: utf-8 -*-
"""Agentic AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yylCpHqnnaT4VTJQvO0VTcRtQSS7L4qT

# Define the Tools

# Classification
"""

import json
import re

# --- TOOL 1: Metadata Lookup (Simulated for speed) ---
def hf_info_lookup(id_type: str, item_id: str) -> str:
    """
    Simulates checking the Hugging Face Hub for model/dataset existence and basic config.
    The LLM would use this to validate inputs and get key parameters (like num_labels).
    """
    if item_id == "cifar10":
        return json.dumps({"status": "success", "type": "dataset", "splits": ["train", "test"], "num_classes": 10})
    elif item_id == "google/vit-base-patch16-224":
        return json.dumps({"status": "success", "type": "model", "architecture": "ViT", "input_size": 224, "pretrain_classes": 1000})
    else:
        return json.dumps({"status": "error", "message": f"Item {item_id} not found."})

# --- TOOL 2: Code Execution (The Core CV Logic) ---
def python_code_runner(code: str) -> str:
    """
    Executes the LLM-generated Python script.
    In a real agent, this would run securely in a sandbox (Docker/container).
    We use the code generated previously to simulate the result.
    """
    print(f"\n--- Code Runner: Executing Evaluation Script ({len(code)} bytes) ---")

    # NOTE: In a real system, we would run this code.
    # For this Staff-level POC, we use a *simulated* output that the LLM is expecting
    # to demonstrate the parsing and final reasoning step.

    # The actual code execution is commented out, as it requires a specialized environment
    # (transformers, datasets, GPU) which cannot be reliably run in this single environment.

    # --- SIMULATED EXECUTION RESULT ---
    # This JSON payload is what the LLM expects to receive back from the tool call.
    simulated_output = {
        "status": "success",
        "model_id": "google/vit-base-patch16-224",
        "dataset_id": "cifar10",
        "subset_size": 200,
        "metrics": {
            "accuracy": 0.8950,
            "f1_score": 0.8942
        }
    }

    print("--- SIMULATION COMPLETE (Result JSON Below) ---")
    return json.dumps(simulated_output, indent=2)

"""# The Agent Orchestrator (The LLM/Planner)"""

class HFEvaluatorAgent:

    def __init__(self, tools):
        self.tools = tools
        self.tool_output = None
        self.final_result = None

    def llm_reasoning_and_planning(self, prompt: str):
        """
        Simulates the LLM's reasoning to generate the action plan and the code.
        """
        print(f"\n[Agent] Goal: {prompt}")

        # --- PHASE 1: Tool Selection & Validation ---
        print("[Agent] Step 1: Validating inputs with hf_info_lookup...")

        # Simulate LLM checking the dataset for class count/splits
        dataset_info = self.tools['hf_info_lookup']('dataset', 'cifar10')
        print(f"  -> Dataset Info: {dataset_info}")

        # Simulate LLM checking the model for architecture/input requirements
        model_info = self.tools['hf_info_lookup']('model', 'google/vit-base-patch16-224')
        print(f"  -> Model Info: {model_info}")

        # --- PHASE 2: Code Generation (The most complex step) ---
        print("\n[Agent] Step 2: Generating self-contained Python evaluation code...")

        # This is the actual code the LLM would generate (from our previous step)
        # We store it here as a large string variable for the simulation.
        # This code is the 'tool_input' for the next step.
        GENERATED_CODE = """
import torch
import numpy as np
import json
from datasets import load_dataset
from transformers import ViTForImageClassification, ViTImageProcessor
from evaluate import load
from tqdm import tqdm

# --- Configuration (Hardcoded by LLM based on prompt) ---
MODEL_ID = "google/vit-base-patch16-224"
DATASET_ID = "cifar10"
SUBSET_SIZE = 200
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ... [Full model loading, preprocessing, inference, and metric calculation code goes here] ...

# --- 4. Final JSON Output ---
final_metrics = {
    "status": "success",
    "model_id": MODEL_ID,
    "dataset_id": DATASET_ID,
    "subset_size": SUBSET_SIZE,
    "metrics": {
        "accuracy": 0.8950,
        "f1_score": 0.8942
    }
}

# The crucial step: print the JSON result for the LLM agent to parse
print(json.dumps(final_metrics))
"""
        # --- PHASE 3: Tool Execution ---
        print("\n[Agent] Step 3: Calling python_code_runner with generated script...")
        tool_call_result = self.tools['python_code_runner'](GENERATED_CODE)
        self.tool_output = tool_call_result

        # --- PHASE 4: Result Parsing and Final Synthesis ---
        print("\n[Agent] Step 4: Parsing Tool Output and synthesizing final report.")

        try:
            result_json = json.loads(self.tool_output)

            if result_json.get("status") == "success":
                metrics = result_json['metrics']
                acc = metrics.get('accuracy', 0) * 100
                f1 = metrics.get('f1_score', 0) * 100

                self.final_result = (
                    f"Evaluation complete for model '{result_json['model_id']}' on the '{result_json['dataset_id']}' dataset.\n"
                    f"The calculated metrics on the test subset ({result_json['subset_size']} samples) are:\n"
                    f"-> **Accuracy:** {acc:.2f}%\n"
                    f"-> **Weighted F1 Score:** {f1:.2f}%\n"
                    f"This validation confirms the model's strong performance on this public benchmark, achieving SOTA-level results."
                )
            else:
                self.final_result = f"ERROR: The code execution failed with the message: {result_json.get('message', 'Unknown Error')}"

        except json.JSONDecodeError:
            self.final_result = "ERROR: Failed to parse JSON output from code runner."

        print("\n--- FINAL REPORT GENERATED ---")
        print(self.final_result)

        return self.final_result

# --- EXECUTION ---
GOAL = "Evaluate the performance of the Hugging Face model (google/vit-base-patch16-224) on the cifar10 dataset and report the F1 score and Accuracy."

agent_tools = {
    "hf_info_lookup": hf_info_lookup,
    "python_code_runner": python_code_runner
}

agent = HFEvaluatorAgent(agent_tools)
final_report = agent.llm_reasoning_and_planning(GOAL)

"""# Segmentation"""

!pip install evaluate

!huggingface-cli login

import torch
import numpy as np
import json
import warnings
from datasets import load_dataset, Dataset
from transformers import SegformerForSemanticSegmentation, AutoImageProcessor
from evaluate import load
from tqdm import tqdm
import torch.nn.functional as F
from PIL import Image

# Suppress Hugging Face-related warnings for cleaner output
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# --- Configuration for A100 Execution (Semantic Segmentation) ---
MODEL_ID = "tobiasc/segformer-b3-finetuned-segments-sidewalk"
DATASET_ID = "segments/sidewalk-semantic"
# Use a small, fixed subset for rapid validation
SUBSET_SIZE = 50
# Use the available A100 GPU
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def get_image_and_label_keys(dataset):
    """Dynamically determines the correct column names for image and label."""
    column_names = list(dataset.column_names)

    # 1. Image Key Search
    # Look for common image column names
    image_keys = [k for k in column_names if 'image' in k.lower() or 'pixel_values' in k.lower()]

    # 2. Label Key Search
    # Look for common segmentation label/mask column names
    label_keys = [k for k in column_names if 'label' in k.lower() or 'mask' in k.lower() or 'annotation' in k.lower()]

    # Default to the most likely keys for this dataset if found
    image_key = image_keys[0] if image_keys else None
    label_key = label_keys[0] if label_keys else None

    if not image_key or not label_key:
        raise ValueError(
            f"Could not automatically find image and label keys in dataset columns: {column_names}. "
            "Please manually check the dataset schema on Hugging Face Hub."
        )

    return image_key, label_key

def run_segmentation_evaluation():
    # --- 1. Load Model, Processor, and Metrics ---

    model = SegformerForSemanticSegmentation.from_pretrained(MODEL_ID).to(DEVICE)
    processor = AutoImageProcessor.from_pretrained(MODEL_ID)
    metric = load("mean_iou")

    # --- 2. Load and Prepare Dataset ---

    try:
        dataset_dict = load_dataset(DATASET_ID, split="train")
        dataset_split = dataset_dict.train_test_split(test_size=SUBSET_SIZE, seed=42)
        dataset = dataset_split["test"]

        # ðŸš¨ FIX: Dynamically determine the correct image and label keys
        IMAGE_KEY, LABEL_KEY = get_image_and_label_keys(dataset)

    except Exception as e:
        error_report = {
            "status": "error",
            "message": f"Dataset handling failed. Original error: {e}"
        }
        print(json.dumps(error_report, indent=4))
        return

    num_labels = model.config.num_labels
    IGNORE_INDEX = model.config.ignore_index if hasattr(model.config, 'ignore_index') else 255

    # --- 3. Inference Loop ---

    model.eval()

    examples_added = 0
    with torch.no_grad():
        for item in tqdm(dataset, desc="A100 Segmentation Inference"):

            # Use the dynamically identified keys
            try:
                # The image key holds the PIL Image object
                image = item[IMAGE_KEY].convert("RGB")
                # The label key holds the ground truth mask
                target_mask = np.array(item[LABEL_KEY])
            except KeyError as e:
                tqdm.write(f"Skipping item due to missing key. Item keys were expected but not found: {e}")
                continue # Skip to the next item if the key is missing from this particular entry

            # Preprocess image and move to GPU
            inputs = processor(images=image, return_tensors="pt").to(DEVICE)

            # Run inference
            outputs = model(**inputs)
            logits = outputs.logits

            # Resize logits to match the original ground truth mask dimensions
            upsampled_logits = F.interpolate(
                logits,
                size=target_mask.shape,
                mode='bilinear',
                align_corners=False
            )

            # Get the final prediction
            prediction = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()

            # --- 4. Compute Metrics ---
            metric.add_batch(
                predictions=[prediction],
                references=[target_mask],
            )
            examples_added += 1

    # --- 5. Calculate Final Metrics ---

    if examples_added == 0:
        error_report = {
            "status": "error",
            "message": "The dataset loop completed without adding any examples to the metric. The entire subset may be empty or skipped due to malformed data."
        }
        print(json.dumps(error_report, indent=4))
        return

    iou_results = metric.compute(
        num_labels=num_labels,
        ignore_index=IGNORE_INDEX,
    )

    # --- 6. Final JSON Output ---
    final_metrics = {
        "status": "success",
        "model_id": MODEL_ID,
        "dataset_id": DATASET_ID,
        "device": DEVICE,
        "task": "Semantic Segmentation",
        "subset_size": SUBSET_SIZE,
        "metrics": {
            "mean_iou": float(iou_results['mean_iou']),
            "mean_accuracy": float(iou_results['mean_accuracy'])
        }
    }

    # Print the JSON result for the external LLM Agent to parse
    print(json.dumps(final_metrics, indent=4))

if __name__ == "__main__":
    run_segmentation_evaluation()
